谷歌TenosrFlow开发者峰会2018上，发布了面向JavaScript开发者的全新机器学习框架 TensorFlow.js

深度学习大致流程

输入数据（图像，目标）和预期的结果
通过神经网络进行大量训练
训练完成就可以识别对应的图像得到预期的结果

卷积神经网络（简称CNN）在图像分类、图像分割、目标检测等领域获得广泛应用。
常用的轻量化网络结构：SqueezeNet、MobileNet、ShuffleNet、Xception
神经网络都是由多层网络组成，如卷积层、池化层、全连接层，
每一层都有若干个节点，称为神经元，每个神经元都是一个带有权重和偏置的函数，它接收一个或多个输入，这些输入乘以被称为“权重”的值并相加，然后，这个值被传递给一个非线性函数，称为激活函数，以生成神经元的输出。
卷积神经网络的神经元不都是全连接的，这区别与一般神经网络。

所谓卷积就是对图像进行卷积计算，简单理解就是矩阵乘法运算，每个卷积层都有若干个卷积核，也叫滤波器，卷积核就是图像处理时，给定输入图像，输入图像中一个小区域中像素加权相乘后再相加成为输出图像中的每个对应像素，其中权值由一个函数定义，这个函数称为卷积核，每个卷积核对应一种图像特征。

神经元和卷积核的关系
一个卷积核是由多个类似于一般神经网络中的神经元组成的，卷积核中的神经元个数，一般有3*3，5*5和7*7，根据经验的验证，3和5是最佳的大小，例如
在单通道图像中，每个卷积核神经元个数是4*4=16个，即每个卷积核就是由16个神经元组成，然后第一层总共有三个卷积核，所以第一层总共有16*3=48个神经元。
在三通道图像中，每个卷积核有三个通道，神经元数量是单通道的三倍，分别对应图像的三个通道，分别进行卷积计算后对应的像素位置值相加得到单通道特征图。
一个卷积核对应一种图像特征，要提取多个图像特征就需要卷积层有多个卷积核，对应神经元的数量也要乘以卷积核数量。
一个卷积神经网络中可能会出现多次卷积层多级池化层，逐步提取图像特征

参考：
https://blog.csdn.net/whr_ws/article/details/82822680
https://zhuanlan.zhihu.com/p/561649075

循环（递归）神经网络（简称RNN）在文本领域广泛应用。

一篇文章让你了解大模型项目的整个研发流程
https://zhuanlan.zhihu.com/p/654733518

如何利用开源大模型（模型基座）训练一个自己的大模型？
SFT（Supervised Finetuning，有监督微调）一种训练方法，当然还有其他方法
1. 模型基座预训练（开源大模型已完成）
2. 词表扩充
开源大模型可能会存在两个问题：
  1 语言不匹配，如[Llama]、[mpt]、[falcon]
  2 专业知识不足
因此需要词表扩充，也就是将一些常见的汉字 token 手动添加到原来的 tokenizer 中
3. 预训练
在扩充完 tokenizer 后，我们就可以开始正式进行模型的预训练步骤了。
预训练的思路很简单，就是输入一堆文本，让模型做 Next Token Prediction 的任务。
预训练过程中所用到的方法：数据源采样、数据预处理、模型结构。
4. 指令微调
  1 Self Instruction 自定义数据
由于预训练任务的本质在于「续写」，而「续写」的方式并一定能够很好的回答用户的问题。
既然模型知道这些知识，只是不符合我们人类的对话习惯，那么我们只要再去教会模型「如何对话」就好了。
既然我们需要去「教会模型说人话」，那么我们就需要去精心编写各式各样人们在对话中可能询问的问题，以及问题的答案。
如果这件事从头开始做自然很难（OpenAI 确实厉害），但今天我们已经有了 ChatGPT 了，Self Instruction 的思路，即通过 ChatGPT 的输入输出来蒸馏自己的模型。
通俗来讲，就是人为的先给一些「训练数据样例」让ChatGPT看，紧接着利用ChatGPT的续写功能，让其不断地举一反三出新的训练数据集。
  2 开源数据集整理
整理开源数据集成问题和答案，然后给大模型进行训练。
5.训练奖励模型Reward Model
奖励模型开源对一个问题的回答进行好坏打分
6.强化学习（Reinforcement Learning，PPO）
利用奖励模型进化微调后的模型，使模型有了区分好坏答案的能力

【LLM】从零开始训练大模型
https://zhuanlan.zhihu.com/p/636270877

关于transform架构
参考：https://zhuanlan.zhihu.com/p/1889229353243627581
整体流程类比CNN网络
cnn是通过卷积核的方式实现权重偏置的计算，y=wk+b，激活，前馈神经网络，反向传播。
transform的attention也是需要用权重代表重要程度，输入*权重=输出，再用输出和label计算loss，transform的权重是通过自注意力机制（Q，K，V）计算得到的。
transformer：输入，位置编码，编码器（attention，前馈网络），解码器（attention，前馈网络，mask），loss，反向传播，优化器。
cnn：输入，卷积核，前馈网络，loss，反向传播，优化器。

Transform 的整体架构，由decoder和encoder构成。构件可以拆解为：
1. 输入嵌入（Input Embedding）: 输入序列首先被转换成固定维度的嵌入向量，这里的embedding是可训的。
2. 位置编码（Positional Encoding）: 由于Transformer不像循环神经网络（RNN）那样自然地处理序列的顺序信息，所以需要添加位置编码以保持序列中单词的位置信息，在Transformer中位置编码不是可训的，是根据位置直接计算的。
3. 多头自注意力机制（Multi-Head Self-Attention）: 允许模型在处理每个序列元素时，同时考虑序列中的所有其他元素，这是通过注意力权重实现的，其中更重要的元素将获得更高的权重。
4. 前馈网络（Feed-Forward Network）: attention模块后接着是一个前馈网络，该网络对每个位置应用相同的全连接层。
5. 残差连接（Residual Connection）和归一化（Normalization）: 在每个子层的输出上，都会进行残差连接，然后在做蹭归一化(Layer-Norm)。
6. 解码器：Transformer模型中的解码器会根据编码器的输出以及之前已生成的输出序列来生成下一个输出。解码器的架构与编码器类似，但它包含一个额外的子层来进行编码器-解码器注意力操作。同时解码器和编码器一样，解码器通常由多个相同的解码层堆叠而成。解码器的遮掩注意力: 防止解码器在生成输出序列时提前“看到”正确答案（后面结合mask原理解释）。
7. 线性层和Softmax: 解码器的最后输出通过一个线性层和Softmax层，将解码器输出转换为预测的下一个词的概率分布。
注意：很多应用transform架构的模型都是多层的，多层有助于更好地理解每个词与其他词之间的关系，上一层的输出是下一层的输入

Attention和Self -Attention的区别
1. Attention:
传统的Attention机制发生在 Target的元素和 Source中的所有元素之间。在一般任务的Encoder-Decoder框架中，输入 Source 和输出 Target 内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子。
2.Self-Attention
Self - Attention 顾名思义，指的不是 Target 和 Source 之间的 Attention 机制，而是 Source 内部元素之间或者 Target 内部元素之间发生的 Attention 机制，其具体计算过程是一样的，只是计算对象发生了变化而已，相当于是 Query=Key=Value，计算过程与attention一样。(例如在Transformer中在计算权重参数时，将文字向量转成对应的 QKV，只需要在 Source 处进行对应的矩阵操作，用不到Target中的信息。)

关于词嵌入Word Embedding
过程：
1.分词
将一句话拆分成一个个token
2.词表查找映射ID
从词表查找token对应的ID
3.将ID映射为语义向量
通过词嵌入模型将ID映射为有意义的向量，需要注意的是对于不同的任务，词嵌入模型也不同，如情感分析，因此词嵌入模型需要根据具体任务进行训练。

作用：向量中的语义距离
词嵌入将文本投影到一个连续向量空间中，有两个重要好处：
相似词距离近：如 “猫” 与 “狗” 的向量距离小
上下文可学习：在上下文中常出现的词向量会被优化为更接近
这使得模型能“理解”词之间的语义关系，而不仅仅是做统计。
参考：https://blog.csdn.net/m0_60610428/article/details/148565685

关于Q,K,V的计算过程
1. 首先得到词嵌入向量，即输入矩阵X
2.提前训练出权重矩阵W_Q,W_K,W_V
3.分别对X和W_Q,W_K,W_V进行叉积得到Q,K,V

注意力权重矩阵计算
计算公式：Attention(Q, K, V) = softmax(Q × K.T / √d) × V
K.T表示K转置，如：[[1,1],[0,1]->[[1,0],[1,1]]
√d表示：K的维度的根号值，如[[1,1],[0,1]，d=2,√2=1.414
softmax（每行分别做 softmax）
最后和V叉积得到Self-Attention的输出
每一层可能有多个Self-Attention，每个头学习不同的“关注方式”，最后把它们拼在一起

关于多头自注意力（Multi-Head Attention）
通过并行多个独立的注意力头，增强模型捕捉不同子空间信息的能力。
 实现方式
将 Q、K、V 拆分为 h 个头。
每个头独立计算注意力，得到 h 个输出。
将多个头的输出拼接后通过线性变换合并。

关于编码器（Encoder）vs 解码器（Decoder）
Transformer 原始的结构有两大块：

编码器（Encoder）：理解输入
解码器（Decoder）：生成输出
比如在翻译任务中：

编码器输入英文句子 “I love you”
解码器输出法语句子 “Je t’aime”
BERT 只用编码器
GPT 只用解码器
原始的 Transformer（用于翻译） 同时用了编码器和解码器





